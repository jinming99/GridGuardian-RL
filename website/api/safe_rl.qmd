---
title: "Safe RL"
sidebar: api
---

## Core Components

### `LagrangianMultiplier`

Adaptive dual variable for constraint satisfaction.

```python
LagrangianMultiplier(
    cost_limit: float,
    variant: str = 'standard',  # 'standard', 'pid', 'adaptive'
    init_value: float = 0.0,
    lr: float = 0.035,
)
```

**Methods:**
- `update(mean_cost: float) -> Dict[str, float]`: Update Î» based on constraint violation
- `get_penalty() -> float`: Current penalty coefficient

**Example:**
```python
lagrange = LagrangianMultiplier(cost_limit=25.0, variant='pid')
for epoch in range(100):
    mean_cost = evaluate_cost(policy)
    stats = lagrange.update(mean_cost)
    penalty = lagrange.get_penalty()
```

---

### `SafeRolloutBuffer`

Extended buffer for constrained MDPs.

```python
SafeRolloutBuffer(
    obs_shape: Tuple,
    act_shape: Tuple,
    capacity: int,
    device: str = 'cpu'
)
```

**Key Methods:**
- `add_safe()`: Store transitions with cost signals
- `compute_returns_and_advantages()`: GAE for rewards and costs

---

### `BaseSafeRLTrainer`

Abstract base for Safe RL algorithms.

```python
class MySafeAlgorithm(BaseSafeRLTrainer):
    def update(self, batch):
        # Implement algorithm-specific update
        pass
```

**Provides:**
- Standardized rollout collection
- Automatic constraint tracking
- Checkpointing infrastructure

---

## Experiment Suite

### `SafeRLExperimentSuite`

Comprehensive evaluation across conditions.

```python
suite = SafeRLExperimentSuite(
    env_factory=make_env,
    cost_limits=[15.0, 25.0, 35.0],
    noise_levels=[0.0, 0.1, 0.2],
    seeds=[0, 1, 2]
)

# Run algorithms
suite.run_algorithm(PPOLag, "PPO-Lag", training_steps=100_000)
suite.run_algorithm(CPO, "CPO", training_steps=100_000)

# Generate report
report = suite.generate_report(include_plots=True)
```

---

## Analysis Functions

### `run_safe_rl_comparison`

Compare algorithms with safety metrics.

```python
results_df = run_safe_rl_comparison(
    algorithms={'PPO-Lag': ppo_lag, 'CPO': cpo},
    env_factory=make_env,
    cost_limit=25.0,
    episodes_per_algorithm=10
)
```

**Returns DataFrame with:**
- `mean_return`, `std_return`
- `mean_cost`, `std_cost` 
- `safety_rate`: Episodes satisfying constraint
- `cvar_95_cost`: Tail risk metric
- `constraint_violation`

---

### `plot_safety_metrics`

Multi-panel safety visualization.

```python
plot_safety_metrics(
    results_df,
    algorithms=['PPO-Lag', 'CPO'],
    cost_limit=25.0
)
```

**Panels:**
1. Cost-Return trade-off
2. Safety rate comparison
3. CVaR analysis
4. Violation distribution
5. Performance stability
6. Composite score

---

### `compute_safety_pareto_frontier`

Estimate cost-reward trade-off.

```python
pareto_df = compute_safety_pareto_frontier(
    policy_fn=policy,
    env_fn=make_env,
    lambda_values=np.logspace(-2, 2, 20)
)
plot_cost_return_pareto(pareto_df)
```

---

## Integration Examples

### Training Loop
```python
lagrange = LagrangianMultiplier(cost_limit=25.0)
buffer = SafeRolloutBuffer(obs_shape, act_shape, 2048)

for epoch in range(100):
    # Collect rollouts
    trajectories = collect_rollouts(policy, env, buffer)
    
    # Update dual variable
    mean_cost = trajectories['costs'].mean()
    lagrange.update(mean_cost)
    
    # Policy update with penalty
    loss = policy_loss - lagrange.get_penalty() * cost_loss
```

### Evaluation
```python
# Create safe policy wrapper
safe_policy = create_safe_policy_wrapper(
    actor_critic,
    deterministic=True,
    normalize_obs=True
)

# Evaluate with safety metrics
mean_ret, std_ret, mean_cost, metrics = evaluate_policy(
    safe_policy,
    make_env,
    track_metrics=['violations']
)
```