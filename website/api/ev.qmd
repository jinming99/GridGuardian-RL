---
title: "EV Utilities"
sidebar: api
---

## Core Functions

### `evaluate_policy`

Comprehensive policy evaluation with metric tracking.

```python
evaluate_policy(
    policy_fn: Callable[[Any], np.ndarray],
    make_env_fn: Callable[..., object],
    episodes: int = 2,
    noise: Optional[float] = None,
    noise_action: Optional[float] = None,
    horizon: int = 288,
    track_metrics: Optional[list[str]] = None,
    verbose: bool = False,
) -> Tuple[float, float, float, Dict[str, Any]]
```

**Parameters:**

| Name | Type | Description |
|------|------|-------------|
| `policy_fn` | `Callable` | Maps observations to actions |
| `make_env_fn` | `Callable` | Environment factory |
| `episodes` | `int` | Evaluation episodes |
| `noise` | `float` | Observation noise [0,1] |
| `noise_action` | `float` | Action noise [0,1] |
| `horizon` | `int` | Max steps (288 = 24h) |
| `track_metrics` | `list[str]` | Metrics to track¹ |

¹ Options: `'satisfaction'`, `'components'`, `'actions'`, `'trajectory'`, `'demands'`

**Example:**
```python
mean_ret, std_ret, mean_cost, metrics = evaluate_policy(
    policy_fn=my_policy,
    make_env_fn=make_env,
    episodes=10,
    track_metrics=['satisfaction']
)
```

---

### `sweep_noise`

Grid search over noise levels for robustness analysis.

```python
sweep_noise(
    policy_fn: Callable,
    make_env_fn: Callable,
    Ns: Iterable[float] = (0.0, 0.05, 0.1, 0.2),
    N_as: Iterable[float] = (0.0, 0.1),
    episodes: int = 2,
) -> pd.DataFrame
```

**Returns:** DataFrame with columns `['noise', 'noise_action', 'return_mean', 'return_std', 'safety_cost']`

---

### `plot_action_distribution`

Comprehensive action distribution analysis.

```python
plot_action_distribution(
    actions: Union[np.ndarray, Dict[str, np.ndarray]],
    title: str = "Action Distribution",
    show_saturation: bool = True,
    saturation_thresholds: Tuple[float, float] = (0.001, 0.999),
    return_stats: bool = True,
) -> Optional[Dict[str, float]]
```

**Features:**
- Histogram with KDE overlay
- Saturation analysis (low/mid/high)
- Statistical summary

---

## Persistence Functions

### `save_timeseries` / `load_timeseries`

```python
save_timeseries(tag: str, ts_df: pd.DataFrame, kind: str = "rl") -> str
load_timeseries(tag: str, kind: str = "rl") -> pd.DataFrame
```

**Cache locations:** `./cache/{kind}/{tag}_timeseries.csv.gz`

### `save_evaluation_results` / `load_evaluation_results`

```python
save_evaluation_results(
    tag: str,
    policy_name: str,
    mean_return: float,
    std_return: float,
    mean_cost: float,
    metrics: Dict[str, Any],
    env_config: Optional[Dict] = None,
    save_full_trajectory: bool = False,
) -> str
```

---

## Visualization Functions

### `plot_robustness_heatmap`
```python
plot_robustness_heatmap(df: pd.DataFrame, value: str = "return_mean")
```

### `plot_training_curves`
```python
plot_training_curves(
    data: Union[pd.DataFrame, Dict[str, pd.DataFrame]],
    x: str = "episode",
    y: str = "reward",
    smoothing_window: int = 200,
    band: str = "std",  # "none", "std", "ci", "quantile"
)
```

### `plot_critical_timeline`
```python
plot_critical_timeline(
    trajectory: Dict[str, np.ndarray],
    env: Any,
    critical_windows: Optional[list] = None,
    topk: int = 3,
)
```

---

## Experiment Management

### `ExperimentTracker`
```python
tracker = ExperimentTracker(base_dir='./experiments')
tracker.add_experiment(tag, config, results_df)
tracker.compare_experiments(['exp1', 'exp2'])
```

### `run_scenario_sweep`
```python
results_df = run_scenario_sweep(
    algorithms={'PPO': ppo_policy, 'SAC': sac_policy},
    env_factory=make_env,
    scenarios={'baseline': {}, 'noisy': {'env_kwargs': {'noise': 0.1}}},
)
```

---

## Multi-Agent Extensions

### `evaluate_multi_agent_policy`
```python
mean_ret, std_ret, mean_cost, metrics = evaluate_multi_agent_policy(
    policy_fn=ma_policy,  # Dict[agent_id, obs] -> Dict[agent_id, action]
    make_env_fn=make_ma_env,
    episodes=10,
    track_metrics=['fairness', 'coordination'],
)
```

### `save_marl_traj`
```python
actions_path, ts_path = save_marl_traj(
    tag="experiment_001",
    traj_actions={'agent_0': actions_0, 'agent_1': actions_1},
    ts=timeseries_df,
)
```