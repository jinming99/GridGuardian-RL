---
title: "RL Infrastructure"
sidebar: api
---

## Data Structures

### `RolloutBuffer`

On-policy trajectory storage for PPO-style algorithms.

```python
RolloutBuffer(
    obs_shape: Tuple,
    act_shape: Tuple, 
    capacity: int,
    device: str = 'cpu'
)
```

**Methods:**
- `add(obs, action, logprob, reward, done, value)`: Store transition
- `get() -> Dict[str, Tensor]`: Return all data for training
- `compute_returns_and_advantages(gamma, gae_lambda)`: GAE computation

**Example:**
```python
buffer = RolloutBuffer((obs_dim,), (act_dim,), capacity=2048)
for step in range(rollout_steps):
    buffer.add(obs, action, logprob, reward, done, value)
batch = buffer.get()
```

---

### `RunningMeanStd`

Online normalization statistics for observation preprocessing.

```python
RunningMeanStd(shape=(), epsilon=1e-4)
```

**Methods:**
- `update(x)`: Update statistics with new batch
- `normalize(x)`: Apply normalization

---

## Network Utilities

### `layer_init`

Orthogonal initialization for neural network layers.

```python
layer_init(layer, std=np.sqrt(2), bias_const=0.0)
```

**Parameters:**
| Name | Type | Description |
|------|------|-------------|
| `layer` | `nn.Module` | PyTorch layer |
| `std` | `float` | Standard deviation for weights |
| `bias_const` | `float` | Constant for bias initialization |

---

### `make_vec_envs`

Create vectorized environments for parallel rollouts.

```python
make_vec_envs(
    env_id: str,
    num_envs: int = 1,
    seed: int = 0,
    device: str = 'cpu',
    wrapper_fn: Optional[Callable] = None
) -> VectorEnv
```

---

## Training Utilities

### `LinearSchedule`

Linear parameter scheduling for learning rates and exploration.

```python
schedule = LinearSchedule(
    initial=1.0,
    final=0.1,
    total_steps=100000
)
current_value = schedule(step)
```

---

### `explained_variance`

Compute explained variance for value function quality assessment.

```python
ev = explained_variance(y_pred, y_true)
# Returns: float in (-inf, 1.0], where 1.0 is perfect prediction
```

---

### `set_random_seed`

Set seeds for reproducibility across all random number generators.

```python
set_random_seed(42)
# Sets: numpy.random, random, torch.manual_seed
```

---

## PPO Components

### Basic Actor-Critic Architecture

```python
class ActorCritic(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_dim=256):
        super().__init__()
        self.actor = nn.Sequential(
            layer_init(nn.Linear(obs_dim, hidden_dim)),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_dim, hidden_dim)),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_dim, act_dim), std=0.01)
        )
        self.critic = nn.Sequential(
            layer_init(nn.Linear(obs_dim, hidden_dim)),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_dim, hidden_dim)),
            nn.Tanh(),
            layer_init(nn.Linear(hidden_dim, 1), std=1.0)
        )
        
    def get_value(self, obs):
        return self.critic(obs)
    
    def get_action_and_value(self, obs):
        mean = self.actor(obs)
        std = torch.ones_like(mean) * 0.5
        dist = Normal(mean, std)
        action = dist.sample()
        return action, dist.log_prob(action).sum(-1), self.critic(obs)
```

---

## Training Loop Utilities

### `run_training_with_monitoring`

Generic training wrapper with logging and checkpointing.

```python
model, results_df = run_training_with_monitoring(
    train_fn=ppo_train_generator,
    config={'lr': 3e-4, 'batch_size': 64},
    tag="experiment_001",
    checkpoint_freq=10000,
    eval_freq=5000
)
```

**Features:**
- Automatic model checkpointing
- Periodic evaluation
- Progress logging to DataFrame

---

## Memory-Based Policies

### LSTM Integration

```python
class LSTMPolicy(nn.Module):
    def __init__(self, obs_dim, act_dim, hidden_dim=256):
        super().__init__()
        self.lstm = nn.LSTM(obs_dim, hidden_dim, batch_first=True)
        self.actor = layer_init(nn.Linear(hidden_dim, act_dim))
        self.critic = layer_init(nn.Linear(hidden_dim, 1))
        
    def forward(self, obs, hidden=None):
        lstm_out, hidden = self.lstm(obs, hidden)
        return self.actor(lstm_out), self.critic(lstm_out), hidden
```

**Usage Notes:**
- Reset hidden state at episode boundaries
- Maintain hidden state across rollout steps
- Use sequence padding for variable-length episodes